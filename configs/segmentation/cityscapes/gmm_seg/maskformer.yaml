BASE_CONFIG: configs/segmentation/cityscapes/gmm_seg/base.yaml

MODEL:
  BACKBONE:
    VERSION: dinov2_vitb14_reg
    FREEZE: True
    INTERM_FEATURES: [1,4,8,11]
    LEARNABLE_PARAMS:
      TYPE: maskformer
      NUM_CLASSES: 19
      PIXEL_DECODER_CONFIG:
        NAME: pixel_decoder
        INPUT_SHAPE: null # dict of name_of_feature -> {stride, channels} If null, then use the hardcoded one (that of dinov2_b)
        CONV_DIM: 256
        MASK_DIM: 256
        NORM: GN
        TRANSFORMER_DROPOUT: 0.0
        TRANSFORMER_NHEADS: 8
        TRANSFORMER_DIM_FEEDFORWARD: 1024
        TRANSFORMER_ENC_LAYERS: 6
        TRANSFORMER_IN_FEATURES: ["res4", "res8", "res11"] # here we need to specify what multiscale features are fed
        COMMON_STRIDE: 4
      TRANSFORMER_PREDICTOR_CONFIG:
        NAME: transformer_decoder
        IN_CHANNELS: 256 #  must be same as the output of the backbone, add consistency rule
        MASK_CLASSIFICATION: True
        NUM_CLASSES: 19
        HIDDEN_DIM: 256
        NUM_QUERIES: 100
        NHEADS: 8
        DIM_FEEDFORWARD: 2048
        DEC_LAYERS: 9
        PRE_NORM: False
        ENFORCE_INPUT_PROJECT: False
        MASK_DIM: 256
        NUM_FEATURES_LEVEL: 3 # must be length of PIXEL_DECODER_CONFIG.TRANSFORMER_IN_FEATURES, add consistency rule
        OOD_PREDICTION: False
  SEGMENTATION_HEAD:
    NAME: MaskHead
    EMBEDDING_DIM: 256
    NUM_CLASSES: 19
  LOSS:
    NAME: maskformer
    NUM_CLASSES: 19
    WEIGHT_DICT:
      loss_ce: 2.0
      loss_mask: 5.0
      loss_dice: 5.0
    LOSSES: [labels, masks]
    EOS_COEF: 0.1
    NUM_POINTS: 12544
    OVERSAMPLE_RATIO: 3.0
    IMPORTANCE_SAMPLE_RATIO: 0.75
